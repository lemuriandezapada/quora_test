{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we start by importing our requirements\n",
    "#Spacy can be pip installed\n",
    "#See how to also install the english model here\n",
    "#https://spacy.io/docs/usage/models\n",
    "import spacy\n",
    "#http://pytorch.org/\n",
    "#Need a gpu for this. Preferably >=8GB ram or some things might not fit on your machine\n",
    "import torch\n",
    "import torch.autograd as ta\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from collections import Counter\n",
    "from sys import stdout\n",
    "\n",
    "\n",
    "from layers import *\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the training data. This is prety straightforward\n",
    "data = [row for row in csv.reader(open('train.csv','r'), delimiter=',')]\n",
    "\n",
    "#spacy does relatively quick parsing, at least for english. Good for a fast and efortless tokenizing of our text\n",
    "#It also comes with Glove's 300d already pretrained wordvectors attached to case insensitive tokens\n",
    "#This comes in handy for our training\n",
    "#We disable the modules we don't directly care about for speed purposes \n",
    "#[it's already really slow because the data is quite big]\n",
    "#fills up to 14GB ram on my laptop, make sure you have at least that :)\n",
    "nlp = spacy.load('en', parser=False, tagger=False, entity=False)\n",
    "\n",
    "\n",
    "#We parse the training sentences, and keep the associated indices and labels\n",
    "indata = [[[y.rank for y in nlp(x[3])],\n",
    "           [y.rank for y in nlp(x[4])]] \n",
    "          for x in data[1:]]\n",
    "\n",
    "outdata = [int(x[5]) for x in data[1:]]\n",
    "outdata = np.array(outdata,dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's extract the top 99 most common characters from the training set\n",
    "#and add a dummy token for the non-trainable, \"masking\" index 0\n",
    "def tokenize(string, chars):\n",
    "    return [chars.index(x) if x in chars else 0 for x in string]\n",
    "\n",
    "#get the most common 99 characters + 1 null token\n",
    "longstring = u''.join([x[3]+x[4] for x in data]).lower()\n",
    "chars = [u'\\x00'] + sorted([x[0] for x in Counter(longstring).most_common(99)])\n",
    "\n",
    "#make the training series of charaters\n",
    "charindata = [[tokenize(x[3], chars), \n",
    "               tokenize(x[4], chars)] \n",
    "              for x in data[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sadly, glove vectors are trained en-masse on corpora irrelevant to our task.\n",
    "#so about 10% of the tokens(words) in the training set do not have vectors.\n",
    "#in principle they would just be masked out to 0, but in our case they can represent specialty topics\n",
    "#so we'll make another series of random vectors (that we will train) out of the top 10k words that glove left behind\n",
    "#this can take a while\n",
    "leftoverdata = [[[y.text for y in nlp(x[3])],\n",
    "                [y.text for y in nlp(x[4])]] \n",
    "                for x in data[1:]]\n",
    "#find out which words don't already have glove vectors via spacy\n",
    "leftovers = [[[y.text.lower() for y in nlp(x[3]) if not y.has_vector] +\n",
    "             [y.text.lower() for y in nlp(x[4]) if not y.has_vector]] \n",
    "             for x in data[1:]]\n",
    "#get the most common 9999 + 1 null token\n",
    "leftovers = [z  for x in leftovers for y in x for z in y]\n",
    "leftovers = [False]+[x[0] for x in Counter(leftovers).most_common()[:9999]]\n",
    "leftovers = {x:leftovers.index(x) for x in leftovers}\n",
    "\n",
    "#make the leftover word indices training series\n",
    "leftoverdata = [[[leftovers[y] if y in leftovers else 0 for y in x[0]],\n",
    "                 [leftovers[y] if y in leftovers else 0 for y in x[1]]] \n",
    "                for x in leftoverdata]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the testing data\n",
    "testdata = [row for row in csv.reader(open('test.csv','r'), delimiter=',')]\n",
    "\n",
    "#make the testing series of glove indices\n",
    "testindata = [[[y.rank for y in nlp(x[1])],\n",
    "               [y.rank for y in nlp(x[2])]] \n",
    "              for x in testdata[1:]]\n",
    "\n",
    "#make the testing series of charaters\n",
    "chartestindata = [[tokenize(x[1], chars), \n",
    "                   tokenize(x[2], chars)] \n",
    "                   for x in testdata[1:]]\n",
    "\n",
    "#make the testing series of leftover word indices\n",
    "leftovertestdata = [[[leftovers[y.text.lower()] if y.text.lower() in leftovers else 0 for y in nlp(x[1])],\n",
    "                   [leftovers[y.text.lower()] if y.text.lower() in leftovers else 0 for y in nlp(x[2])]] \n",
    "                   for x in testdata[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For GPU training purposes, it's best that we use GPU based embedded vector indices to minimize data transfers by \n",
    "#about 300%. For that we set up a function to retrieve the vocabulary weight matrix and transfer it to the GPU later\n",
    "def get_embeddings(vocab):\n",
    "    max_rank = max(lex.rank for lex in vocab if lex.has_vector)\n",
    "    vectors = np.ndarray((max_rank+1, vocab.vectors_length), dtype='float32')\n",
    "    for lex in vocab:\n",
    "        if lex.has_vector:\n",
    "            vectors[lex.rank] = lex.vector\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We might have some memory limitations, so we set an upper limit for how many wordvectors we transfer\n",
    "#total spacy glove WV size is about ~740k, so not that big of a difference\n",
    "#a lot is junk but I won't filter it out here. No time.\n",
    "\n",
    "embsize = 700000\n",
    "#We create a separate embedding model on the GPU and feed its outputs to the siamese network.\n",
    "#It makes it easier to make sure we keep these weights frozen this way, as this can be excluded from the optimizer\n",
    "embedder = nn.Embedding(embsize, 300).cuda()\n",
    "\n",
    "#we override the random weights with the ones we stole from spacy :>\n",
    "#I just assume you have a GPU\n",
    "embedder.weight.data = torch.Tensor(get_embeddings(nlp.vocab)[:embsize]).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def call_prediction(model, inseq0, inseq1, linseq0, linseq1, cinseq0, cinseq1, volatile = False):\n",
    "    #get the trainable embeddings for the input sequences    \n",
    "    tinseq0 = trainable_embedder(ta.Variable(torch.from_numpy(inseq0).cuda(), volatile = volatile))\n",
    "    tinseq1 = trainable_embedder(ta.Variable(torch.from_numpy(inseq1).cuda(), volatile = volatile))    \n",
    "    #get the embeddings for the input sequences    \n",
    "    inseq0 = embedder(ta.Variable(torch.from_numpy(inseq0).cuda(), volatile = volatile))\n",
    "    inseq1 = embedder(ta.Variable(torch.from_numpy(inseq1).cuda(), volatile = volatile))\n",
    "    #get the embeddings for the leftover input sequences\n",
    "    linseq0 = extraembedder(ta.Variable(torch.from_numpy(linseq0).cuda(), volatile = volatile))\n",
    "    linseq1 = extraembedder(ta.Variable(torch.from_numpy(linseq1).cuda(), volatile = volatile))\n",
    "    \n",
    "    #transfer the indices for the character sequences\n",
    "    cinseq0 = ta.Variable(torch.from_numpy(cinseq0).cuda(), volatile = volatile)\n",
    "    cinseq1 = ta.Variable(torch.from_numpy(cinseq1).cuda(), volatile = volatile)\n",
    "    \n",
    "    #create an initial states variable for the recurrent layers\n",
    "    h_0 = ta.Variable(torch.zeros(1, inseq0.size(0), 256).cuda(), volatile = volatile)\n",
    "    \n",
    "    #concatenate all the embeddings together:\n",
    "    #300d fixed from glove\n",
    "    #50d trainable from us for domain adaptation\n",
    "    #50d for leftover words\n",
    "    #not more, we don't have enough data or the right training scheme to train proper \"full\" vectors for them\n",
    "    #so we want to limit their capacity\n",
    "    #400d total, each branch\n",
    "    inseq0 = torch.cat([inseq0, tinseq0, linseq0],2)\n",
    "    inseq1 = torch.cat([inseq1, tinseq1, linseq1],2)\n",
    "    #get the prediction back and return it\n",
    "    pred = model(inseq0, inseq1, cinseq0, cinseq1, h_0)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sadly I don't have time to go through all of the many tunable parameters \n",
    "#One can try to work network depth, dropout rate, input transformations, number of units, etc\n",
    "#One single model so far gets a LB entry of about .35 Not to bad for just dumping in data, but not imppressive either\n",
    "#(others claim more with similar architectures)\n",
    "#A quick, ML approved hack is to do model averaging of different models trained with different parameters\n",
    "#We don't have time to explore many parameter settings with those either :)\n",
    "#So we'll just vary the optimizer and the data partition\n",
    "#This should bias each of our models towards a local gradient minimum and a local data representation minimum\n",
    "#So we'll just make a bunch of optimizer styles that we'll just iterate through\n",
    "optnames = ['Adam', 'Adamax', 'Adadelta', 'Adagrad', 'ASGD', 'SGD', 'RMSprop']\n",
    "def optmaker(name):\n",
    "    ensembleopts = {'Adam':opt.Adam(itertools.chain(model.parameters(),\n",
    "                                         extraembedder.parameters(), \n",
    "                                         trainable_embedder.parameters()),\n",
    "                         weight_decay = 0.0001),\n",
    "                    'Adamax':opt.Adamax(itertools.chain(model.parameters(),\n",
    "                                         extraembedder.parameters(), \n",
    "                                         trainable_embedder.parameters()),\n",
    "                         weight_decay = 0.0001),\n",
    "                    'Adadelta':opt.Adadelta(itertools.chain(model.parameters(),\n",
    "                                         extraembedder.parameters(), \n",
    "                                         trainable_embedder.parameters()),\n",
    "                         weight_decay = 0.0001),\n",
    "                    'Adagrad':opt.Adagrad(itertools.chain(model.parameters(),\n",
    "                                         extraembedder.parameters(), \n",
    "                                         trainable_embedder.parameters()),\n",
    "                         weight_decay = 0.0001),\n",
    "                    'ASGD':opt.ASGD(itertools.chain(model.parameters(),\n",
    "                                         extraembedder.parameters(), \n",
    "                                         trainable_embedder.parameters()),\n",
    "                         weight_decay = 0.0001),\n",
    "                    'SGD':opt.SGD(itertools.chain(model.parameters(),\n",
    "                                         extraembedder.parameters(), \n",
    "                                         trainable_embedder.parameters()),\n",
    "                            lr = 1e-2,\n",
    "                            momentum = 0.9,\n",
    "                            nesterov = True,\n",
    "                            weight_decay = 0.0001),\n",
    "                    'RMSprop':opt.RMSprop(itertools.chain(model.parameters(),\n",
    "                                         extraembedder.parameters(), \n",
    "                                         trainable_embedder.parameters()),\n",
    "                        lr = 1e-4,\n",
    "                         weight_decay = 0.0001),                    \n",
    "                   }\n",
    "    return ensembleopts[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We create some training data indices\n",
    "sequence = np.arange(len(indata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's iterate thrgouh our optimizers\n",
    "for optname in optnames:\n",
    "    #Make our siamese model and trainable embedding layers\n",
    "    model = Siamese().cuda()\n",
    "    trainable_embedder = nn.Embedding(embsize, 50, padding_idx = 0).cuda()\n",
    "    extraembedder = nn.Embedding(len(leftovers), 50, padding_idx = 0).cuda()\n",
    "    #create the optimizer with the above's parameters\n",
    "    optimizer = optmaker(optname)\n",
    "    #Shuffle the training data and split it up 90% training, 10% validation\n",
    "    np.random.shuffle(sequence)\n",
    "    trainsequence = sequence[:int(sequence.shape[0]*.9)]\n",
    "    validsequence = sequence[int(sequence.shape[0]*.9):]    \n",
    "    \n",
    "    #Set the batch size to whatever fits into memory [no more than 64 for such a big network with the 300x700k matrix :)\n",
    "    batch_size = 64\n",
    "    #We'll do just 10 epochs each\n",
    "    for z in range(10):\n",
    "        print('===========')\n",
    "        #Shuffle the training sequence for random sequetial ordering, and varrying padding lengths\n",
    "        np.random.shuffle(trainsequence)\n",
    "        #initialize print losses and counters\n",
    "        cnt = 0\n",
    "        avgpredloss = 0\n",
    "        \n",
    "        #set model into training mode\n",
    "        #This enables dropout, batchnorm, etc\n",
    "        model.train()\n",
    "        #start the training loop\n",
    "        #Note, due to rounding errors, up to batch_size - 1 data points might get omitted in the loop.\n",
    "        #This shouldn't impact training much in this case, as they would get picked up in the next batch\n",
    "        #But in a \"proper\" deployment implementation it should be taken care of \n",
    "        #[or just treat it like more regularization] :]\n",
    "        for k in range(len(trainsequence)//batch_size):\n",
    "            cnt += batch_size\n",
    "            #select a random batch of training data, inputs and labels\n",
    "            randselect = trainsequence[k*batch_size:k*batch_size+batch_size]\n",
    "            outdataset = np.expand_dims(outdata[trainsequence[k*batch_size:k*batch_size+batch_size]],1)\n",
    "\n",
    "            #Convert it into a feedable format and generate the input sequences\n",
    "            inset = [indata[i] for i in randselect]\n",
    "            charinset = [charindata[i] for i in randselect]\n",
    "            leftoverinset = [leftoverdata[i] for i in randselect]\n",
    "            (inseq0, inseq1, \n",
    "             linseq0, linseq1, \n",
    "             cinseq0, cinseq1) = generate_inseqs(embsize, inset, leftoverinset, charinset)\n",
    "\n",
    "            #add some noise to the data. Randomly omit some words/letters (by mapping them to the null one)\n",
    "            #This case is a 1% chance for each\n",
    "            #A number too low and the network will overfit as it specializes on certain obvious features\n",
    "            #A number too high and you don't learn any details\n",
    "            inseq0 = addnoise(inseq0, 0.01)\n",
    "            inseq1 = addnoise(inseq1, 0.01)\n",
    "            linseq0 = addnoise(linseq0, 0.01)\n",
    "            linseq1 = addnoise(linseq1, 0.01)\n",
    "            cinseq0 = addnoise(cinseq0, 0.01)\n",
    "            cinseq1 = addnoise(cinseq1, 0.01)\n",
    "\n",
    "\n",
    "            #get the embeddings for the input sequences\n",
    "            pred = call_prediction(model, inseq0, inseq1, linseq0, linseq1, cinseq0, cinseq1, False)\n",
    "\n",
    "            #set the targets, compute the weighted loss and optimize an iteration\n",
    "            targ = ta.Variable(torch.Tensor(1.*outdataset)).cuda()\n",
    "            #this is a bit of a cheat as you're not supposed to look at the test set. \n",
    "            #But I'll assume that kaggle \"cheated\" as the training and test sets are supposed to be drawn from\n",
    "            #similar distributions\n",
    "            #An all 1 submission yields a loss of 28.52, an all 0 submission yields a loss 6.01\n",
    "            #This indicates a roughly 20-80 positive-negative split\n",
    "            #These guys say about 17%\n",
    "            #https://www.kaggle.com/c/quora-question-pairs/discussion/31179\n",
    "            #So to counter that we'll scale our crossentropy function to give more weight to negative examples\n",
    "            #as the training set is more of a 40-60 split. \n",
    "            #Either that or we oversample the negatives, but that's just a waste of flops           \n",
    "            loss = torch.mean(-( 0.472 * targ * torch.log(pred) + 1.309 * (1.0 - targ) * torch.log(1.0 - pred) ))\n",
    "            #do the backwards pass, and update the weights and lcear the buffers\n",
    "            loss.backward(retain_variables=False)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #do some printing of average training losses to keep track of our progress\n",
    "            avgpredloss += loss.data.cpu().numpy()[0] * batch_size\n",
    "            stdout.write(\"\\r%d %f\" % (cnt, avgpredloss/cnt))\n",
    "            stdout.flush()\n",
    "        avgpredloss /= cnt\n",
    "        stdout.write(\"\\n\")\n",
    "        \n",
    "\n",
    "        #Done training one epoch, time for testing\n",
    "        cnt = 0\n",
    "        validloss = 0\n",
    "        #Set the model into evaluation mode. This disables dropout, etc\n",
    "        model.eval()\n",
    "        #Note: Here as well. Some testing points might get lost. Given the proportionally large number of remaining\n",
    "        #samples, the effect should be minimal. But in a proper data science analysis, this should be taken care of.\n",
    "        #we do it here due to lack of time, don't do it at home :)\n",
    "        for k in range(len(validsequence)//batch_size):\n",
    "            cnt += batch_size\n",
    "            #Testing loop is similar to training out of laziness and lack of time to make it pretty\n",
    "            #There's no actual need for random sequential order for evaluation. So this time it's continuous\n",
    "            #but the split is pre-shuffled once\n",
    "            randselect = validsequence[k*batch_size:k*batch_size+batch_size]\n",
    "            outdataset = np.expand_dims(outdata[validsequence[k*batch_size:k*batch_size+batch_size]],1)\n",
    "\n",
    "            #Generate the sequences, feed them through the net and retrieve the prediction\n",
    "            inset = [indata[k] for k in randselect]\n",
    "            charinset = [charindata[i] for i in randselect]\n",
    "            leftoverinset = [leftoverdata[i] for i in randselect]\n",
    "            \n",
    "            (inseq0, inseq1, \n",
    "             linseq0, linseq1, \n",
    "             cinseq0, cinseq1) = generate_inseqs(embsize, inset, leftoverinset, charinset) \n",
    "            \n",
    "            pred = call_prediction(model, inseq0, inseq1, linseq0, linseq1, cinseq0, cinseq1, True)\n",
    "\n",
    "            #Compute the loss and print it out\n",
    "            #There's no backward pass here\n",
    "            targ = ta.Variable(torch.Tensor(1.*outdataset)).cuda()\n",
    "            predloss = torch.mean(-( 0.472 * targ * torch.log(pred) + 1.309 * (1.0 - targ) * torch.log(1.0 - pred)))\n",
    "            \n",
    "            #integrate loss and print\n",
    "            validloss += predloss.data.cpu().numpy()[0]*batch_size\n",
    "            stdout.write(\"\\r%d %f\" % (cnt, validloss/cnt))\n",
    "            stdout.flush()\n",
    "        predloss /= cnt\n",
    "        stdout.write(\"\\n\")\n",
    "    \n",
    "    #Ok. Measurable things are done, time to run it on kaggle's data\n",
    "    #make some empty keepers\n",
    "    results = np.empty([0,1])\n",
    "    cnt = 0\n",
    "    #eval mode is already set but just to remeind ourselves\n",
    "    model.eval()\n",
    "    for k in range(len(testindata)//batch_size):\n",
    "        #iterate through the test data, generate the sequences, feed them to the net, \n",
    "        #get the predictions and annex it to the list\n",
    "        cnt += batch_size\n",
    "        inset = testindata[k*batch_size:k*batch_size+batch_size]\n",
    "        charinset = chartestindata[k*batch_size:k*batch_size+batch_size]\n",
    "        leftoverinset = leftovertestdata[k*batch_size:k*batch_size+batch_size]\n",
    "        (inseq0, inseq1, \n",
    "         linseq0, linseq1,\n",
    "         cinseq0, cinseq1) = generate_inseqs(embsize, inset, leftoverinset, charinset) \n",
    "\n",
    "        pred = call_prediction(model, inseq0, inseq1, linseq0, linseq1, cinseq0, cinseq1, True)\n",
    "\n",
    "        #add the predictions to the result\n",
    "        results = np.vstack([results,pred.data.cpu().numpy()])\n",
    "        \n",
    "        stdout.write(\"\\r%d\" % cnt)\n",
    "        stdout.flush()\n",
    "\n",
    "    #Do the last batch in case the dataset is not 128 dvisible\n",
    "    inset = testindata[k*batch_size+batch_size:]\n",
    "    charinset = chartestindata[k*batch_size+batch_size:]\n",
    "    leftoverinset = leftovertestdata[k*batch_size+batch_size:]\n",
    "    (inseq0, inseq1, \n",
    "     linseq0, linseq1, \n",
    "     cinseq0, cinseq1) = generate_inseqs(embsize, inset, leftoverinset, charinset) \n",
    "    pred = call_prediction(model, inseq0, inseq1, linseq0, linseq1, cinseq0, cinseq1, True)\n",
    "    results = np.vstack([results,pred.data.cpu().numpy()])\n",
    "\n",
    "    #write out the results to a CSV to be compounded together later\n",
    "    #I don't save the models because it's not so straightforward, and I'm running low on disk space\n",
    "    #But you can if you want\n",
    "    finaldata = [['test_id','is_duplicate']]+[[x[0],y[0]] for x,y in zip(testdata[1:],results)]\n",
    "    csv.writer(open(optname+\"_\"+str(avgpredloss)+\"_\"+str(validloss)+'submission.csv','w')).writerows(finaldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
